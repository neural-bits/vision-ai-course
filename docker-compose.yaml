version: '3.4'

services:
  triton_server:
    container_name: triton-inference-server-23.10
    image: nvcr.io/nvidia/tritonserver:23.10-py3  
    privileged: true
    ports:
      - 8001:8001
      - 8002:8002
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    volumes:
      - ${PWD}/model_repository:/models
    command: ["tritonserver", "--model-repository=/models"] 